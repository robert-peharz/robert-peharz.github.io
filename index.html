<!DOCTYPE html>
<html>

    <head>
        <meta charset="utf-8"/>
        <title>Robert Peharz</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        
        <!-- Font Awesome Icons -->
        <link rel="stylesheet" href="css/font-awesome.min.css"/>
        
        <!-- Bootstrap -->
        <link href="css/bootstrap.min.css" rel="stylesheet"/>
        <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->
        
        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
        <script src="js/html5shiv.js"></script>
        <script src="js/respond.min.js"></script>
        <![endif]-->
        
        
        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <!-- http://getbootstrap.com/customize/ change "@screen-sm" -> "@screen-md" in @grid-float-breakpoint -->
        <script src="js/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="js/bootstrap.min.js"></script>
        <script src="js/menucollapse.js"></script>
        <script type="text/javascript" src="js/arrow78.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>
    </head>




    <body id="page-top" class="index">

        <!-- Navigation -->
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"  data-target="#bs-example-navbar-collapse-2" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="glyphicon glyphicon-search"></span>
                    </button>
                    <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <span><a href="#"><img border="0" width="130" src="images/TU_Graz_logo.png"/></a></span>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav navbar-right">

                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();" href="#aboutme">About Me</a>
                        </li>
                        <li>
                            <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=ywkqnqMAAAAJ">Publications</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-news-content').show();" href="#news">News</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-phds').show();" href="#phds">PhD Students</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-projects-content').show();" href="#projects">Recent Paper Highlights</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-tutorials-content').show();" href="#tutorials">Tutorials</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-projects-content').show();" href="#academic_service">Academic Service</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-intro-content').show();" href="#bio">Short Bio</a>
                        </li>
                    </ul>
                </div><!-- /.navbar-collapse -->

            </div><!-- /.container-fluid -->
        </nav>


        <!-- Home Section -->
        <section  name="aboutme" id="aboutme" style="margin-top:40px">
            <div class="container">
                <hr class="star-primary">

                <div align="left" class="col-md-4">
                    <a target="_blank" href="images/profile.jpg">
                        <img id="mobile-img" src="images/profile.png" width="335" border="0" height="340" alt="">
                    </a>
                    <br>
                    <br>
                    <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=ywkqnqMAAAAJ"><img src="images/scholar.png" width="25" border="0" alt=""></a>
                    <a target="_blank" href="https://www.linkedin.com/in/robert-peharz-278108330/"><img id="linkedin_logo" src="images/linkedin_logo.png" width="30" border="0" alt=""></a>
                    <a target="_blank" href="https://twitter.com/ropeharz"><img id="twitter_logo" src="images/twitter_logo.png" width="30" border="0" alt=""></a>
                    <a href="mailto:robert.peharz@tugraz.at"><img src="images/mail_logo.png" width="30" border="0" alt=""></a>
                </div>

                <div align="left" class="col-md-5">
                    <h2><b>Robert Peharz</b></h2>

                    <h4><b>Assistant Professor at TUG</b><br></h4>

                    <h4>
                        <b>
                            <i>
                                AI, probabilistic machine learning, inference, tractability,
                                causality, active learning, Bayesian optimization, experimental design,
                                physics-informed learning, neurosymbolic learning
                            </i>
                        </b> <br>
                    </h4>

                    <h4>Formerly
                        <ul>
                            <li>Assistant Professor at TU Eindhoven (NL)</li>
                            <li>Marie-Curie Individual Fellow at University of Cambridge (UK)</li>
                            <li>Postdoc at Medical University Graz</li>
                        </ul>
                        <br>
                    </h4>



                    
                </div>

                <!--<p><a href="#" class="btn btn-primary">More &raquo;</a></p>-->
                <b class="expandshow" style="display:none;" id="tab-1">
                    <button class="btn btn-primary btn-lg btn-block" style="width: 100%;" onclick="javascript:$('.allshow').show();$('.noshow').hide();$('.collapseshow').show();$('.expandshow').hide();">Expand all Sections<span class="glyphicon glyphicon-chevron-down"></span></button>
                </b>
                <b class="collapseshow" style="display:none;" id="tab-2">
                    <button class="btn btn-primary btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('.allshow').hide();$('.noshow').show();$('.expandshow').show();$('.collapseshow').hide();">Collapse all Sections<span class="glyphicon glyphicon-chevron-up"></span></button>
                </b>
                
            </div>
        </section>


        <!-- Hiring Section -->
        <section name="hiring" id="hiring">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-hiring">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-intro-content').toggle();">I'm Hiring!<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-hiring-content">
                        <hr class="star-primary">

                        <h2 style="color:Red;">I'm Hiring! (2 positions)</h2>
                        <div>
                            I'm looking for a highly motivated PhD student who is keen to work in the intersection of
                            <b>causality, tractable probabilistic models and logic</b>.
                            The position is part of the prestigious Cluster of Excellence
                            <a target="_blank" href="https://www.bilateral-ai.net//"><b>Bilateral AI</b></a>, led
                            by
                            <a target="_blank" href="https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/sepp-hochreiter/">
                                Sepp Hochreiter
                            </a>
                            and funded by the
                            <a target="_blank" href="https://www.fwf.ac.at/">Austrian Science Fund</a>.

                            <br>
                            <br>
                            <i>
                            Broad AI aims at solving a wide array of problems, rather than being limited to a single
                            task or domain. By combining sub-symbolic AI (machine learning, ML) with symbolic AI
                            (knowledge representation and reasoning, KRR), Bilateral AI provides the means to develop
                            the foundations of the capabilities and skill acquisition for problem solving by a Broad AI.
                            </i>
                            <br>
                            <br>

                            Position advertised via
                            <a target="_blank" href="https://ellis.eu/news/ellis-phd-program-call-for-applications-2024">ELLIS</a>.
                            Further applicant info <a target="_blank" href="https://www.tugraz.at/institute/igi/institute/open-positions/bilai-jobs">here</a>.

                            <br>
                            <br>
                            <br>
                            Furthermore, I'm looking for a highly motivated and creative PhD student in the areas
                            <b>computer vision, visual object detection, probabilistic machine learning and neuro-symbolic AI, </b>
                            who will contribute to an EIC funded project on the design of DNA-based data storage systems:
                            <a target="_blank" href="https://neodna.eu/">NEO DNA</a>.

                            <br>
                            <a target="_blank" href="https://jobs.tugraz.at/en/jobs">https://jobs.tugraz.at/en/jobs</a>

                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!--              -->
        <!-- News Section -->
        <!--              -->
        <section  name="news" id="news">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-5">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-news-content').toggle();">News<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-news-content">
                        <hr class="star-primary">

                        <h2>News</h2>
                        <ul>
                            <li>
                                I am invited to give a lecture at the
                                <a target="_blank" href="https://genu.ai/2024/">
                                    <b><i>Generative Models and Uncertainty Quantification</i></b>
                                </a>
                                workshop in Copenhagen. Here are the
                                <a target="_blank" href="doc/GenU2024.pdf">
                                    <b><i>slides</i></b>.
                                </a>
                                <p style="color:#808080";>(September 2024)</p>
                            </li>
                            <li>
                                Honored to be a finalist for TUG's <b>Excellent Teaching Award</b> for
                                <b>all 3 of my courses</b> (Machine Learning 1, Reinforcement
                                Learning, and Probabilistic Machine Learning)!
                                <!-- <span style="color: Blue"> -->
                                    <b>This honor is awarded to less than 1% of all eligible courses at TUG!</b>
                                <!-- </span> -->
                                <br>
                                <a target="_blank" href="images/Pfel.jpg">
                                    <img id="Pfel" src="images/Pfel.jpg" width="500" border="0" alt="">
                                </a>
                                <p style="color:#808080";>(November 2023)</p>
                            </li>
                            <li>
                                As a continuation of last year, Antonio Vergari and I gave a 5-day course on
                                <b><i>Probabilistic Circuits</i></b> at the
                                <a target="_blank" href="https://essai2024.di.uoa.gr/">
                                    <b><i>2nd European Summer School on Artificial Intelligence (ESSAI)</i></b>
                                </a>
                                in Athens. Here are the
                                  <a target="_blank" href="https://github.com/smatmo/ESSAI24-PCs">
                                    <b><i>slides</i></b>.
                                  </a>
                                <p style="color:#808080";>(August 2023)</p>
                            </li>
                            <li>
                                Paper accepted at <b>ICML:</b>
                                <a target="_blank" href="https://openreview.net/forum?id=0mklK4h0rX">
                                    <b><i>Exact Soft Analytical Side-Channel Attacks using Tractable Circuits.</i></b>
                                </a>
                                With Thomas Wedenig, Rishub Nagpal, Gaëtan Cassiers, Stefan Mangard.
                                <p style="color:#808080";>(May 2024)</p>
                            </li>
                            <li>
                                Paper accepted at <b>NeurIPS (oral!):</b>
                                <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/f4b768188be63b8d2680a46934fd295a-Abstract-Conference.html">
                                    <b><i>How to Turn Your Knowledge Graph Embeddings into Generative Models.</i></b>
                                </a>
                                With Lorenzo Loconte, Nicola Di Mauro, and Antonio Vergari
                                <p style="color:#808080";>(October 2023)</p>
                            </li>
                            <li>
                                Spend a week at the
                                <a target="_blank" href="https://simons.berkeley.edu/workshops/probabilistic-circuits-logic">
                                    <b><i>Probabilistic Circuits and Logic</i></b>
                                </a>
                                workshop, at the
                                <a target="_blank" href="https://simons.berkeley.edu/">
                                    <b><i>Simons Institute (Berkeley)</i></b>.
                                </a>
                                I talked about crypto side-channel attacks and other recent projects.
                                Here are the
                                <a target="_blank" href="doc/logic_pcs.pdf">
                                    <b><i>slides</i></b>.
                                </a>
                                <p style="color:#808080";>(October 2023)</p>
                            </li>
                            <li>
                                New project started!
                                <a target="_blank" href="https://neodna.eu/">
                                    <b><i>NEO</i></b>
                                </a> funded by the <b>European Innovation Council (EIC)</b>.
                                Next generation DNA data-storage powered by neurosymbolic AI.
                                <br>
                                <a target="_blank" href="images/fundedEU.png">
                                    <img id="funded-EU-NEO" src="images/fundedEU.png" width="300" border="0" alt="">
                                </a>
                                <p style="color:#808080";>(October 2023)</p>
                            </li>
                            <li>
                                <a target="_blank" href="https://www.bmbwf.gv.at/Ministerium/Der-Bundesminister.html">
                                    <b><i>Minister Martin Polaschek</i></b>
                                </a>
                                (Education, Science and Research) visited our
                                <a target="_blank" href="https://www.vanillaflow.eu/">
                                    <b><i>VanillaFlow</i></b>
                                </a> project.
                                <a target="_blank" href="images/visit_minister.jpg">
                                    <img id="polaschek-img" src="images/visit_minister.jpg" width="700" border="0" height="326" alt="">
                                </a>
                                <p style="color:#808080";>(September 2023)</p>
                            </li>
                            <li>
                                New project started!
                                <a target="_blank" href="https://www.vanillaflow.eu/">
                                    <b><i>VanillaFlow</i></b>
                                </a> funded by the <b>European Innovation Council (EIC)</b>.
                                AI-guided development of novel vanillin based molecules for redox flow batteries.
                                <br>
                                <a target="_blank" href="images/profile.jpg">
                                    <img id="funded-EU-VanillaFlow" src="images/fundedEU.png" width="300" border="0" alt="">
                                </a>
                                <p style="color:#808080";>(September 2023)</p>
                            </li>
                            <li>
                                Antonio Vergari and I gave a 5-day course on <b><i>Probabilistic Circuits</i></b> at the
                                <a target="_blank" href="https://essai.si/">
                                    <b><i>1st European Summer School on Artificial Intelligence (ESSAI)</i></b>
                                </a>
                                in Ljubljana. The course was recorded and can be watched
                                  <a target="_blank" href="https://videolectures.net/ESSAIandACAI2023_peharz_vergari_reasoning/">
                                    <b><i>here</i></b>.
                                  </a>
                                <p style="color:#808080";>(August 2023)</p>
                            </li>
                            <li>
                                I was invited to give a lecture on <b><i>Probabilistic Circuits</i></b> at the
                                <a target="_blank" href="https://gemss.ai/">
                                    <b><i>Generative Modeling Summer School</i></b>
                                </a>
                                in Copenhagen. Here are the
                                <a target="_blank" href="doc/GeMSS23ProbabilisticCircuits.pdf">
                                    <b><i>slides</i></b>.
                                </a>
                                <p style="color:#808080";>(June 2023)</p>
                            </li>
                            <li>
                                Our paper
                                <a target="_blank" href="https://proceedings.mlr.press/v206/yang23a.html">
                                    <b><i>Bayesian Structure Scores for Probabilistic Circuits</i></b>
                                </a>
                                has been accepted at <b>AISTATS</b>.
                                With
                                <i>Yang Yang and Gennaro Gala.</i>
                                <p style="color:#808080";>(January 2023)</p>
                            </li>

                            <li>
                                Our paper
                                <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/25883">
                                    <b><i>Continuous mixtures of tractable probabilistic models</i></b>
                                </a>
                                has been accepted at <b>AAAI</b>.
                                With
                                <i>Alvaro Correia, Gennaro Gala, Eric Quaeghebeur, Cassio de Campos.</i>
                                <p style="color:#808080";>(November 2022)</p>
                            </li>

                            <li>
                                Our paper
                                 <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/675e371eeeea99551ce47797ed6ed33e-Abstract-Conference.html">
                                 <b><i>Active Bayesian Causal Inference</i></b>
                                 </a>
                                 has been accepted at <b>NeurIPS</b>.
                                 With
                                 <i>Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf, and Julius von Kügelgen.</i>
                                 <p style="color:#808080";>(September 2022)</p>
                            </li>

                            <li>
                                I will give a new version of our
                                <a target="_blank" href="https://nips.cc/virtual/2022/tutorial/55809">
                                    <b><i>Tutorial on Probabilistic Circuits</i></b>
                                </a>
                                at <b>NeurIPS</b>,
                                together with Antonio Vergari, YooJung Choi, and Guy Van den Broeck.
                                 (watch it here) </a>
                                <p style="color:#808080";>(August 2022)</p>
                            </li>
                            <li>
                                We will have a virtual workshop on
                                <a target="_blank" href="https://ncsi.cause-lab.net/">
                                    <b><i>Neuro Causal and Symbolic AI</i></b>b>
                                </a>
                                at NeurIPS</b>.
                                With Devendra Singh Dhami (TU Darmstadt, hessian.AI), Christina Winkler (TU München),
                                Thomas Kipf (Google Brain), Matej Zečević (TU Darmstadt),
                                Petar Veličković (DeepMind, University of Cambridge), and
                                Kristian Kersting (TU Darmstadt, hessian.AI)
                                <p style="color:#808080";>(July 2022)</p>
                            </li>
                            <li>
                                Our paper
                                <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0888613X21001766">
                                    <b><i>Conditional sum-product networks: Modular probabilistic circuits via gate functions</i></b>
                                </a>
                                has been published in IJAR.
                                With
                                <i> Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Thomas Liebig, and Kristian Kersting </i>
                                <p style="color:#808080";>(January 2022)</p>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>



        <!--                -->
        <!-- Research Group -->
        <!--                -->
        <section  name="phds" id="phds">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-9">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-phds').toggle();">PhD Students<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-phds">
                        <hr class="star-primary">

                        <h2>PhD Students</h2>
                        <h3>Current</h3>
                        <ul>
                            <li>
                                Sepideh Adamiat (co-supervision with Wouter Kouw and Bert de Vries, TU Eindhoven,
                                <a target="_blank" href="https://www.tue.nl/en/our-university/calendar-and-events/academic-events/tue-science-awards/tue-science-award-team-science-nominees/bayesbrain">
                                    <b><i>BayesBrain project</i></b>
                                </a>)
                            </li>

                            <li>
                                Irina Dobrianski (main supervisor, neurosymbolic AI,
                                <a target="_blank" href="https://neodna.eu/">
                                    <b><i>NEO project</i></b>
                                </a>)
                            </li>
                            <li>
                                Johannes Exenberger (co-supervision with Gerald Scheiger, physics-informed ML)
                            </li>
                            <li>
                                Giacomo Di Gobbi (main supervisor, ML for molecules, Bayesian optimization,
                                <a target="_blank" href="https://www.vanillaflow.eu/">
                                    <b><i>VanillaFlow project</i></b>
                                </a>)
                            </li>
                            <li>
                                Tim d'Hondt (co-supervision with Mykola Pechenizkiy, TU Eindhoven, federated learning)
                            </li>
                            <li>
                                Christian Toth (co-supervision with Franz Pernkopf, causal models)
                            </li>
                            <li>
                                Thomas Wedenig (main supervisor, neurosymbolic AI, circuits)
                            </li>
                        </ul>
                        <h3>Previous</h3>
                        <ul>
                            <li>
                                Alvaro Correia (co-supervision with Cassio de Campos, probabilistic circuits)
                            </li>
                            <li>
                                Martin Trapp (co-supervision with Franz Pernkopf, probabilistic circuits)
                            </li>
                            <li>
                                David Montalvan (main supervisor, probabilistic machine learning, generative models)
                            </li>
                        </ul>

                    </div>
                </div>
            </div>
        </section>


        <!-- Research Section -->
        <section  name="projects" id="projects">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-6">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-projects-content').toggle();">Recent Paper Highlights<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-projects-content">
                        <hr class="star-primary">

                        <h2>Recent Paper Highlights</h2>

                        <br>
                        <br>


                        <img id="exsasca-pics" src="images/exsasca_pics.png" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">ICML'24</span>
                        <b>Exact Soft Analytical Side-Channel Attacks using Tractable Circuits</b>
                        <br>
                        Thomas Wedenig, Rishub Nagpal, Gaëtan Cassiers, Stefan Mangard, Robert Peharz

                        <br>
                        <br>
                        Cryptographic algorithms are safe&mdash;unless they executed on a physical device...
                        A prominent attack in information security is the <b>soft analytic side channel attack (SASCA)</b>
                        which assumes that the attacker has access to a copy of the device under attack.
                        With this, the attacker can develop so-called templates, i.e. statistical models predicting
                        intermediate computational results from physical side channels (power trace, temperature,
                        electromagnetic emissions, timing, etc.).
                        These "soft guesses" about intermediate results, together with the logical knowledge about
                        the cryptoalgorithm can be used to infer the secret key with astonishing high success rate.
                        Basically, one is facing a complicated and high-dimensional probabilistic-logic inference
                        problem, which is usually approximated with loopy belief propagation.
                        In this paper, we show that, surprisingly, one can solve such challenging inference problems
                        even <b>exactly</b> using the power of tractable circuits!
                        Results are substantially increased top-1 success rates, higher noise resilience
                        (demonstrating increase vulnerability of protected implementations) and a proof-of-concept
                        for future attack scenarios.
                        <br>
                        <a target="_blank" href="https://proceedings.mlr.press/v235/wedenig24a.html">Abstract</a> |
                        <a target="_blank" href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/wedenig24a/wedenig24a.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/wedenigt/exsasca">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>




                        <img id="KGE-pics" src="images/gekcs_pics.png" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">NeurIPS'23</span> <span style="color:Red;"><b>oral!</b></span>
                        <b>How to Turn Your Knowledge Graph Embeddings into Generative Models</b>
                        <br>
                        Lorenzo Loconte, Nicola Di Mauro, Robert Peharz, Antonio Vergari

                        <br>
                        <br>
                        Knowledge graphs (KGs) are probably the most prominent way to represent structured domain information
                        and are heavily used in services by big players such as Goolge and Amazon.
                        KGs are basically directed multi-graphs on <i>subjects</i> and <i>objects</i> where links
                        in the graph correspond to <i>predicates</i>.
                        Knowledge graph embeddings (KGEs) represent KGs (<b>symbolic</b>) as real- or complex-valued
                        vectors (<b>sub-symbolic</b>), allowing to link KGs with powerful deep learning approaches.
                        It turns out that the most common KGEs can be interpreted as (probabilistic) circuits!
                        This opens the door for generative training, efficient inference and neuro-symbolic tricks, such
                        as incorporating hard logical knowledge in our KGs.

                        <br>
                        <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/f4b768188be63b8d2680a46934fd295a-Abstract-Conference.html">Abstract</a> |
                        <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/f4b768188be63b8d2680a46934fd295a-Paper-Conference.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/april-tools/gekcs">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>


                        <img id="abci-pics" src="images/ABCI_pics.png" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">NeurIPS'22</span>
                        <b>Active Bayesian Causal Inference</b>
                        <br>
                        Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf, Robert Peharz, Julius Von Kügelgen

                        <br>
                        <br>
                        Since thousands of years, humans wonder about the nature of causality.
                        From a technical perspective, this question becomes relatively benign if we put forward a
                        mathematical model of causality, such as a structural causal model (SCM).
                        In a nutshell, an SCM is just a dependency network among quantities, where the value of each
                        quantity is determined as a function of other quantities and exogenous noise.
                        Thus, an SCM is specified via a directed acyclic graph (DAG), a set of mechanisms (functions doing the
                        assignment) and a distribution over exogenous variables.
                        If we completely know the SCM, then we can answer causal queries on all levels of Pearl's
                        hierarchy, that is (i) observational, (ii) interventional and (iii) counterfactual queries.
                        <b>Problem: we don't know the SCM.</b> Very often we know even almost nothing about it.
                        Thus, at its core, causal reasoning is busy with an epistemic challenge, i.e. how much do we
                        know about the model and how can we work with that.
                        Whenever there is a epistemic uncertainty, the <b>natural Bayesian reflex</b> is "I put a prior on this
                        and infer the posterior".
                        In this paper, we are going full Bayesian over entire SCMs and develop methods to handle the
                        "computational nightmare" which usually comes with Bayesian computation.
                        Wonderful features of this approach is that we can directly infer causal queries without
                        fully knowing the SCM, but uncertainty estimates on them, and use Bayesian epistemic
                        uncertainties to actively collect interventional data, in order to answer our causal questions
                        with as little data as possible.

                        <br>
                        <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/675e371eeeea99551ce47797ed6ed33e-Abstract-Conference.html">Abstract</a> |
                        <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/675e371eeeea99551ce47797ed6ed33e-Paper-Conference.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/chritoth/active-bayesian-causal-inference">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>


                        <img id="gef-pics" src="images/gef_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <span class="label label-success">NeurIPS'20</span>
                        <b>Joints in Random Forests</b>
                        <br>
                        Alvaro Correia, Robert Peharz, Cassio P. de Campos

                        <br>
                        <br>
                        Decision trees and random forests are some of the most widely used machine learning models,
                        and random forests are one of the strongest classifiers on tabular data.
                        But did you know that there was always a generative model hiding in your random forest?
                        Here we show how to exploit this fact for little extra resources.
                        Specifically, we show how decision trees can be translated into probabilistic circuits (PCs), and random forests
                        into an ensemble of PCs.
                        This generalizes the possibilities of standard random forests, such as consistent treatment of
                        missing data (by probabilistic inference) and outlier detection.

                        <br>
                        <a target="_blank" href="https://proceedings.neurips.cc//paper/2020/hash/8396b14c5dff55d13eea57487bf8ed26-Abstract.html">Abstract</a> |
                        <a target="_blank" href="https://proceedings.neurips.cc/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.com/38936595">Video</a> |
                        <a target="_blank" href="https://github.com/AlCorreia/GeFs">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>



                        <img id="einet-pics" src="images/einet_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <span class="label label-success">ICML'20</span>
                        <b>Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits</b>
                        <br>
                        Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van Den Broeck, Kristian Kersting, Zoubin Ghahramani
                        <br>
                        <br>
                        Probabilistic circuits are tractable probabilistic models and allow exact and efficient inference.
                        However, they used to be slow in comparison to deep neural networks, since their special structure
                        (which makes them tractable in the first place) does not map nicely onto deep learning frameworks
                        such as PyTorch or Tensorflow. Here we proposed a "smart" implementation of PCs, by squeezing all PC
                        operations into one a handful large <i>einsum operations</i>.
                        The result: dramatic speedups and memory savings, large-scale
                        generative modeling, including data imputation and outlier detection.
                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v119/peharz20a.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v119/peharz20a/peharz20a.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.com/38928148">Video</a> |
                        <a target="_blank" href="https://github.com/cambridge-mlg/EinsumNetworks">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>



                        <img id="dsmgp-pics" src="images/dsmgp_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <span class="label label-success">AISTATS'20</span>
                        <b>Deep Structured Mixtures of Gaussian Processes</b>
                        <br>
                        Martin Trapp, Robert Peharz, Franz Pernkopf, Carl Edward Rasmussen

                        <br>
                        <br>
                        Gaussian processes (GPs) are a powerful tool for Bayesian regression, as they represent a prior
                        over functions, which gets updated to a posterior via Bayesian inference.
                        Interestingly, this Bayesian update is tractable as it takes cubic time and quadratic memory.
                        While polynomial, this complexity is still prohibitive for large data, i.e. a few thousand data points.
                        Here we marry GPs with probabilistic circuits (PCs), yielding <i>Deep Structured Mixture of Gaussian Processes (DSMGPs),</i>
                        a new process model which elegantly mixes the tractable inference mechanisms of GPs and PCs.
                        The new model fits data better than several GP approximations while having comparable runtimes.
                        DSMGPs are also more data efficient than these approximate techniques and allow to model heteroscedastic noise.

                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v108/trapp20a.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v108/trapp20a/trapp20a.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.at/38930154">Video</a> |
                        <a target="_blank" href="https://github.com/trappmartin/DeepStructuredMixtures">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>




                        <img id="ps3-pics" src="images/ps3_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">ECML PKDD'20</span>
                        <b>PS3: Batch Mode Active Learning for Hate Speech Detection in Imbalanced Text Data</b>
                        <br>
                        Ricky Maulana Fajri, Samaneh Khoshrou, Robert Peharz, Mykola Pechenizkiy

                        <br>
                        <br>
                        The steadily growing prominence of social media exacerbates the problem of hostile
                        contents and hate-speech.
                        Automatically recognizing hate-speech is difficult, since the difference between hate-speech
                        and non-hate-speech might be subtle.
                        Moreover, hate-speech is relatively rare, leading to a highly class-skewed problem.
                        We developed PS3, a simple and effective batch mode active learning solution, which
                        updates the detection system by querying human domain-experts to annotate carefully selected
                        batches of data instances.
                        Despite its simplicity, PS3 sets state-of-the art on several hate-speech datasets.

                        <br>
                        <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-67670-4_5">Abstract</a> |
                        <a target="_blank" href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-67670-4_5.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.com/38932384">Video</a> |
                        <a target="_blank" href="https://github.com/rmfajri/PS3">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>



                        <img id="spvae-pics" src="images/spvae_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">ICML'19</span>
                        <b>Hierarchical Decompositional Mixtures of Variational Autoencoders</b>
                        <br>
                        Ping Liang Tan, Robert Peharz

                        <br>
                        <br>
                        Variational autoencoders (VAEs) are simple and powerful neural density estimators and have
                        received a lot of attention recently.
                        However, inference and learning in VAEs is still challenging due to the intractable nature of the
                        model, especially in high dimensional data spaces.
                        Here we propose a <i>divide-and-conquer approach</i> and break up the overall density estimation
                        problem into many sub-problems, which are each modeled with a set of "small VAEs."
                        Learning and inference in these VAE components are orchestrated via probabilistic circuits
                        (PCs), yielding hierarchical decompositional mixtures of VAEs.
                        This novel model effectively uses <i>hybrid exact-approximate inference</i> (exact from PCs,
                        approximate from VAEs) in a natural way.
                        We show that our model outperforms classical VAEs on almost all of our experimental
                        benchmarks.
                        Moreover, we show that our model is highly data efficient and degrades very gracefully in
                        extremely low data regimes.


                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v97/tan19b.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v97/tan19b/tan19b.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/cambridge-mlg/SPVAE">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>




                        <img id="ratspn-pics" src="images/ratspn_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">UAI'19</span>
                        <b>Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning</b>
                        <br>
                        Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp, Kristian Kersting, Zoubin Ghahramani

                        <br>
                        <br>
                        Probabilistic circuits (PCs) such as sum-product networks (SPNs) are expressive probabilistic
                        models with a rich set of exact and efficient inference routines.
                        Their structure, however, does not easily map to deep learning frameworks such as Tensorflow.
                        Here we use an unspecialized random SPN structure which maps easily onto these frameworks and
                        can be scale to millions of parameters.
                        These <i>Random and Tensorized SPNs (RAT-SPNs)</i> perform often en par with state-of-the-art
                        neural net learners and deep neural networks on a diverse range of generative and discriminative
                        tasks.
                        RAT-SPNs can be used to naturally treat missing data and for outlier analysis and detection.
                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v115/peharz20a.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v115/peharz20a/peharz20a.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/cambridge-mlg/RAT-SPN">Code</a>


                    </div>
                </div>
            </div>
        </section>



        <!-- Tutorial Section -->
        <section name="tutorials" id="tutorials">
             <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                     <b class="noshow" style="display:none;" id="tab-7">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-tutorials-content').toggle();">Tutorials<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-tutorials-content">
                        <hr class="star-primary">
                        <h2>Tutorials on Probabilistic Circuits</h2>

                        Exact and efficient probabilistic inference and learning are becoming more and more mandatory when we want to quickly take complex decisions in presence of uncertainty in real-world scenarios where approximations are not a viable option. In this tutorial, we will introduce probabilistic circuits (PCs) as a unified computational framework to represent and learn deep probabilistic models guaranteeing tractable inference. Differently from other deep neural estimators such as variational autoencoders and normalizing flows, PCs enable large classes of tractable inference with little or no compromise in terms of model expressiveness. Moreover, after showing a unified view to learn PCs from data and several real-world applications, we will cast many popular tractable models in the framework of PCs while leveraging it to theoretically trace the boundaries of tractable probabilistic inference.

                        <br>

                        <br>
                        <span class="label label-success">NeuriPS'22</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Applications </b>
                        <br>
                        <a target="_blank" href="https://nips.cc/virtual/2022/tutorial/55809">Website</a>

                        <br>
                        <span class="label label-success">IJCAI'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="https://ijcai20.org/tutorials/">Website</a>

                        <br>
                        <span class="label label-success">ECAI'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="https://digital.ecai2020.eu/session/probabilistic-circuits-representations-inference-learning-and-applications/">Website</a> |
                        <a target="_blank" href="http://web.cs.ucla.edu/~guyvdb/slides/ECAI20.pdf">Slides</a>

                        <br>
                        <span class="label label-success">ECML PKDD'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="http://web.cs.ucla.edu/~guyvdb/talks/ECML-PKDD20-tutorial/">Website</a> |
                        <a target="_blank" href="https://www.youtube.com/watch?v=2RAG5-L9R70">Video</a> |
                        <a target="_blank" href="http://web.cs.ucla.edu/~guyvdb/slides/ECML20.pdf">Slides</a>

                        <br>
                        <span class="label label-success">AAAI'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="https://aaai.org/Conferences/AAAI-20/aaai20tutorials/">Website</a> |
                        <a target="_blank" href="http://starai.cs.ucla.edu/slides/AAAI20.pdf">Slides</a>


                    </div>
                </div>
             </div>
        </section>


        <!-- Academic Service Section -->
        <section  name="projects" id="academic_service">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-8">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-academic-service-content').toggle();">Academic Service<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-academic-service-content">
                        <hr class="star-primary">

                        <h2>Academic Service</h2>

                        <br>

                        <ul>
                            <li>
                                <b>Area Chair</b><br>
                                <ul>
                                    <li>
                                        UAI (2022)
                                    </li>
                                    <li>
                                        ECML/PKDD (2022)
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <b>Senior Committee Member</b><br>
                                <ul>
                                    <li>
                                        UAI (2021)
                                    </li>
                                    <li>
                                        IJCAI (2019, 2020)
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <b>Reviewer</b><br>
                                <ul>
                                    <li>
                                        ICML (2013, 2014, 2019, 2020 [top 33%])
                                    </li>
                                    <li>
                                        NeurIPS (2018 [top 30%], 2019 [top 50%])
                                    </li>
                                    <li>
                                        AAAI (2019, 2021)
                                    </li>
                                    <li>
                                        IJCAI-ECAI (2018 [top 11.5%])
                                    </li>
                                    <li>
                                        UAI (2020)
                                    </li>
                                    <li>
                                        CVPR (2015, 2016, 2017, 2018)
                                    </li>
                                    <li>
                                        ICCV/ECCV (2015, 2016, 2018)
                                    </li>
                                    <li>
                                        PGM (2020)
                                    </li>
                                    <li>
                                        Interspeech, ICASSP (2013, 2014, 2016)
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <b>Reviewed for Journals</b><br>
                                <ul>
                                    <li>
                                        Transactions on Artificial Intelligence
                                    </li>
                                    <li>
                                        Journal of Machine Learning Research
                                    </li>
                                    <li>
                                        IEEE Transactions on Audio, Speech and Language Processing
                                    </li>
                                    <li>
                                        Data Mining and Knowledge Discovery, Springer
                                    </li>
                                    <li>
                                        Machine Learning, Springer
                                    </li>
                                    <li>
                                        PeerJ
                                    </li>
                                    <li>
                                        Neural Computation, The MIT Press
                                    </li>
                                    <li>
                                        International Journal of Approximate Reasoning, Elsevier
                                    </li>
                                    <li>
                                        Expert Systems With Applications, Elsevier
                                    </li>
                                    <li>
                                        Neurocomputing, Elsevier
                                    </li>
                                    <li>
                                        Signal Processing, Elsevier
                                    </li>
                                    <li>
                                        Computational Optimization and Applications, Springer
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <b>Reviewed for Funding Agencies</b><br>
                                <ul>
                                    <li>
                                        Deutsche Forschungsgemeinschaft (DFG)
                                    </li>
                                </ul>
                            </li>
                        </ul>

                    </div>
                </div>
            </div>
        </section>


        <!-- Bio Section -->
        <section name="bio" id="bio">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-3">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-intro-content').toggle();">Short Bio<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-intro-content">
                        <hr class="star-primary">

                         <h2>Short Bio</h2>
                        <div>
                            Robert Peharz received his PhD from TU Graz (Austria) in
                            2015. In his PhD he was working on probabilistic graphical models and probabilistic circuits
                            (sum-product networks), with applications to signal processing.
                            He was a postdoc in the
                            <a target="_blank" href="https://www.medunigraz.at/idn/">iDN group</a>
                             at the Medical University of Graz (Austria), working on interdisciplinary
                            approaches for early recognition of neural maldevelopment via behavioral neuroscience.
                            From 2017-2018 he was a postdoc in the
                            <a target="_blank" href="http://mlg.eng.cam.ac.uk/">Machine Learning Group (MLG)</a>
                            at the University of Cambridge
                            and a Marie-Curie Individual Fellow at MLG Cambridge from 2018-2019.
                            From 2019-2021 he was an Assistant Professor at the Eindhoven University of Technology
                            (TU/e, Netherlands).
                            Currently, he is an Assistant Professor at Graz University of Technology.
                        </div>

                    </div>

                </div>
            </div>
        </section>


        <br>
        <br>
        <br>
        <br>
        <br>
        <hr class="star-primary">
        <footer>
            <small>
                <center>
                    © 2015 | D. Zeinalipour. Credits: AR template
                    <a onclick="javascript:$('#credit').toggle();"><img border="0" src="images/ccby.png"/></a>
                    <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
                        <a href="https://github.com/dmsl/academic-responsive-template" target="_blank">
                            https://github.com/dmsl/academic-responsive-template
                        </a> ]
                    </div>
                </center>
            </small>
        </footer>
        
    </body>
    
</html>
