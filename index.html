<!DOCTYPE html>
<html>

    <head>
        <meta charset="utf-8"/>
        <title>Robert Peharz</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
        
        <!-- Font Awesome Icons -->
        <link rel="stylesheet" href="css/font-awesome.min.css"/>
        
        <!-- Bootstrap -->
        <link href="css/bootstrap.min.css" rel="stylesheet"/>
        <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->
        
        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
        <script src="js/html5shiv.js"></script>
        <script src="js/respond.min.js"></script>
        <![endif]-->
        
        
        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <!-- http://getbootstrap.com/customize/ change "@screen-sm" -> "@screen-md" in @grid-float-breakpoint -->
        <script src="js/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="js/bootstrap.min.js"></script>
        <script src="js/menucollapse.js"></script>
        <script type="text/javascript" src="js/arrow78.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>
    </head>




    <body id="page-top" class="index">

        <!-- Navigation -->
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"  data-target="#bs-example-navbar-collapse-2" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="glyphicon glyphicon-search"></span>
                    </button>
                    <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <span><a href="#"><img border="0" width="130" src="images/TU_Graz_logo.png"/></a></span>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav navbar-right">

                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();" href="#aboutme">About Me</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-intro-content').show();" href="#bio">Short Bio</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-news-content').show();" href="#news">News</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-publications-content').show();" href="#publications">Publications</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-tutorials-content').show();" href="#tutorials">Tutorials</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-projects-content').show();" href="#projects">Recent Projects</a>
                        </li>
                        <li class="page-scroll">
                            <a onclick="javascript:reset_menus();$('#tab-projects-content').show();" href="#academic_service">Academic Service</a>
                        </li>
                    </ul>
                </div><!-- /.navbar-collapse -->

            </div><!-- /.container-fluid -->
        </nav>


        <!-- Home Section -->
        <section  name="aboutme" id="aboutme" style="margin-top:40px">
            <div class="container">
                <hr class="star-primary">

                <div align="left" class="col-md-3">
                    <a target="_blank" href="images/profile.jpg">
                        <img id="mobile-img" src="images/profile.jpg" width="257" border="0" height="340" alt="">
                    </a>
                </div>

                <div align="left" class="col-md-5">
                    <h2>Robert Peharz</h2>

                    <h4>Assistant Professor<br>
                        <a target="_blank" href="https://www.tugraz.at/home/">Graz University of Technology</a>
                        <br>
                        <a target="_blank" href="https://www.tugraz.at/institute/igi/institute/">Institute of Theoretical Computer Science</a>
                        <br>
                         <!--
                         P.O. Box 20537<br>
                         1678 Nicosia, CYPRUS<br>-->
                        <!--
                        Tel/Fax: <a href="tel:0035722892755">+357-22892755</a> / <a href="tel:0035722892701">01</a><br/>
                        -->
                        Email: robert.peharz at tugraz.at<br>
                        Twitter: <a target="_blank" href="https://twitter.com/ropeharz">@ropeharz</a>
                        <!--Directions: <a target="_blank" href="https://goo.gl/FQCHMl">http://goo.gl/FQCHMl</a>-->
                    </h4>
                    <h4>Formerly postdoc at Medical University Graz (AUT), Marie-Curie Individual Fellow at University of Cambridge (UK), Assistant Professor at TU Eindhoven (NL) <br></h4>
                    <h4>
                        <b>
                            <i>
                                artificial intelligence, inference, probabilistic learning, tractability,
                                decision making, causality, active learning, Bayesian optimization, experimental
                                design
                            </i>
                        </b> <br>
                    </h4>


                    
                </div>

                <!--<p><a href="#" class="btn btn-primary">More &raquo;</a></p>-->
                <b class="expandshow" style="display:none;" id="tab-1">
                    <button class="btn btn-primary btn-lg btn-block" style="width: 100%;" onclick="javascript:$('.allshow').show();$('.noshow').hide();$('.collapseshow').show();$('.expandshow').hide();">Expand all Sections<span class="glyphicon glyphicon-chevron-down"></span></button>
                </b>
                <b class="collapseshow" style="display:none;" id="tab-2">
                    <button class="btn btn-primary btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('.allshow').hide();$('.noshow').show();$('.expandshow').show();$('.collapseshow').hide();">Collapse all Sections<span class="glyphicon glyphicon-chevron-up"></span></button>
                </b>
                
            </div>
        </section>




        <!-- Bio Section -->
        <section name="bio" id="bio">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-3">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-intro-content').toggle();">Short Bio<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>
                    
                    <div class="allshow" style="display:none;" id="tab-intro-content">
                        <hr class="star-primary">

                         <h2>Short Bio</h2>
                        <div>
                            Robert Peharz received his PhD from TU Graz (Austria) in
                            2015. In his PhD he was working on probabilistic graphical models and probabilistic circuits
                            (sum-product networks), with applications to signal processing.
                            He was a postdoc in the
                            <a target="_blank" href="https://www.medunigraz.at/idn/">iDN group</a>
                             at the Medical University of Graz (Austria), working on interdisciplinary
                            approaches for early recognition of neural maldevelopment via behavioral neuroscience.
                            From 2017-2018 he was a postdoc in the
                            <a target="_blank" href="http://mlg.eng.cam.ac.uk/">Machine Learning Group (MLG)</a>
                            at the University of Cambridge
                            and a Marie-Curie Individual Fellow at MLG Cambridge from 2018-2019.
                            From 2019-2021 he was an Assistant Professor at the Eindhoven University of Technology
                            (TU/e, Netherlands).
                            Currently, he is an Assistant Professor at Graz University of Technology.
                        </div>

                    </div>
                    
                </div>
            </div>
        </section>



         <!-- News Section -->
        <section  name="news" id="news">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-5">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-news-content').toggle();">News<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-news-content">
                        <hr class="star-primary">

                        <h2>News</h2>
                        <ul>
                            <li>
                                I was invited to give a lecture on <b><i>Probabilistic Circuits</i></b> at the
                                <a target="_blank" href="https://gemss.ai/">
                                    <b><i>Generative Modeling Summer School</i></b>
                                </a>
                                in Copenhagen. Here are the
                                  <a target="_blank" href="doc/GeMSS23ProbabilisticCircuits.pdf">
                                    <b><i>slides</i></b>!
                                </a>
                                <p style="color:#808080";>(June 2023)</p>
                            </li>
                            <li>
                                Our paper
                                <a target="_blank" href="https://proceedings.mlr.press/v206/yang23a.html">
                                    <b><i>Bayesian Structure Scores for Probabilistic Circuits</i></b>
                                </a>
                                has been accepted at <b>AISTATS</b>.
                                With
                                <i>Yang Yang and Gennaro Gala.</i>
                                <p style="color:#808080";>(January 2023)</p>
                            </li>

                            <li>
                                Our paper
                                <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/25883">
                                    <b><i>Continuous mixtures of tractable probabilistic models</i></b>
                                </a>
                                has been accepted at <b>AAAI</b>.
                                With
                                <i>Alvaro Correia, Gennaro Gala, Eric Quaeghebeur, Cassio de Campos.</i>
                                <p style="color:#808080";>(November 2022)</p>
                            </li>

                            <li>
                                Our paper
                                 <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/675e371eeeea99551ce47797ed6ed33e-Abstract-Conference.html">
                                 <b><i>Active Bayesian Causal Inference</i></b>
                                 </a>
                                 has been accepted at <b>NeurIPS</b>.
                                 With
                                 <i>Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf, and Julius von Kügelgen.</i>
                                 <p style="color:#808080";>(September 2022)</p>
                            </li>

                            <li>
                                I will give a new version of our
                                <a target="_blank" href="https://nips.cc/virtual/2022/tutorial/55809">
                                    <b><i>Tutorial on Probabilistic Circuits</i></b>
                                </a>
                                at <b>NeurIPS</b>,
                                together with Antonio Vergari, YooJung Choi, and Guy Van den Broeck.
                                 (watch it here) </a>
                                <p style="color:#808080";>(August 2022)</p>
                            </li>
                            <li>
                                We will have a virtual workshop on
                                <a target="_blank" href="https://ncsi.cause-lab.net/">
                                    <b><i>Neuro Causal and Symbolic AI</i>
                                </a>
                                at NeurIPS</b>.
                                With Devendra Singh Dhami (TU Darmstadt, hessian.AI), Christina Winkler (TU München),
                                Thomas Kipf (Google Brain), Matej Zečević (TU Darmstadt),
                                Petar Veličković (DeepMind, University of Cambridge), and
                                Kristian Kersting (TU Darmstadt, hessian.AI)
                                <p style="color:#808080";>(July 2022)</p>
                            </li>
                            <li>
                                Our paper
                                <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0888613X21001766">
                                    <b><i>Conditional sum-product networks: Modular probabilistic circuits via gate functions</i></b>
                                </a>
                                has been published in IJAR.
                                With
                                <i> Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Thomas Liebig, and Kristian Kersting </i>
                                <p style="color:#808080";>(January 2022)</p>
                            </li> 
                        </ul>
                    </div>
                </div>
            </div>
        </section>






        <!-- Research Section -->
        <section  name="publications" id="publications">
             <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                     <b class="noshow" style="display:none;" id="tab-4">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-publications-content').toggle();">Publications<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-publications-content">
                        <hr class="star-primary">
                        <h2>Publications</h2>
                        For a full list, please see my
                        <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=ywkqnqMAAAAJ"> Google Scholar profile </a>
                    </div>
                </div>
             </div>
        </section>




         <!-- Tutorial Section -->
        <section name="tutorials" id="tutorials">
             <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                     <b class="noshow" style="display:none;" id="tab-7">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-tutorials-content').toggle();">Tutorials<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-tutorials-content">
                        <hr class="star-primary">
                        <h2>Tutorials on Probabilistic Circuits</h2>

                        Exact and efficient probabilistic inference and learning are becoming more and more mandatory when we want to quickly take complex decisions in presence of uncertainty in real-world scenarios where approximations are not a viable option. In this tutorial, we will introduce probabilistic circuits (PCs) as a unified computational framework to represent and learn deep probabilistic models guaranteeing tractable inference. Differently from other deep neural estimators such as variational autoencoders and normalizing flows, PCs enable large classes of tractable inference with little or no compromise in terms of model expressiveness. Moreover, after showing a unified view to learn PCs from data and several real-world applications, we will cast many popular tractable models in the framework of PCs while leveraging it to theoretically trace the boundaries of tractable probabilistic inference.

                        <br>

                        <br>
                        <span class="label label-success">NeuriPS'22</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Applications </b>
                        <br>
                        <a target="_blank" href="https://nips.cc/virtual/2022/tutorial/55809">Website</a>

                        <br>
                        <span class="label label-success">IJCAI'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="https://ijcai20.org/tutorials/">Website</a>

                        <br>
                        <span class="label label-success">ECAI'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="https://digital.ecai2020.eu/session/probabilistic-circuits-representations-inference-learning-and-applications/">Website</a> |
                        <a target="_blank" href="http://web.cs.ucla.edu/~guyvdb/slides/ECAI20.pdf">Slides</a>

                        <br>
                        <span class="label label-success">ECML PKDD'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="http://web.cs.ucla.edu/~guyvdb/talks/ECML-PKDD20-tutorial/">Website</a> |
                        <a target="_blank" href="https://www.youtube.com/watch?v=2RAG5-L9R70">Video</a> |
                        <a target="_blank" href="http://web.cs.ucla.edu/~guyvdb/slides/ECML20.pdf">Slides</a>

                        <br>
                        <span class="label label-success">AAAI'20</span>
                        <b>Probabilistic Circuits: Representations, Inference, Learning and Theory</b>
                        <br>
                        <a target="_blank" href="https://aaai.org/Conferences/AAAI-20/aaai20tutorials/">Website</a> |
                        <a target="_blank" href="http://starai.cs.ucla.edu/slides/AAAI20.pdf">Slides</a>


                    </div>
                </div>
             </div>
        </section>



        <!-- Research Section -->
        <section  name="projects" id="projects">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-6">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-projects-content').toggle();">Recent Projects<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-projects-content">
                        <hr class="star-primary">

                        <h2>Recent Projects</h2>

                        <br>


                        <img id="gef-pics" src="images/gef_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <span class="label label-success">NeurIPS'20</span>
                        <b>Joints in Random Forests</b>
                        <br>
                        Alvaro Correia, Robert Peharz, Cassio P. de Campos

                        <br>
                        <br>
                        Decision trees and random forests are some of the most widely used machine learning models,
                        and random forests are one of the strongest classifiers on tabular data.
                        But did you know that there was always a generative model hiding in your random forest?
                        Here we show how to exploit this fact for little extra resources.
                        Specifically, we show how decision trees can be translated into probabilistic circuits (PCs), and random forests
                        into an ensemble of PCs.
                        This generalizes the possibilities of standard random forests, such as consistent treatment of
                        missing data (by probabilistic inference) and outlier detection.

                        <br>
                        <a target="_blank" href="https://proceedings.neurips.cc//paper/2020/hash/8396b14c5dff55d13eea57487bf8ed26-Abstract.html">Abstract</a> |
                        <a target="_blank" href="https://proceedings.neurips.cc/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.com/38936595">Video</a> |
                        <a target="_blank" href="https://github.com/AlCorreia/GeFs">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>



                        <img id="einet-pics" src="images/einet_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <span class="label label-success">ICML'20</span>
                        <b>Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits</b>
                        <br>
                        Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van Den Broeck, Kristian Kersting, Zoubin Ghahramani
                        <br>
                        <br>
                        Probabilistic circuits are tractable probabilistic models and allow exact and efficient inference.
                        However, they used to be slow in comparison to deep neural networks, since their special structure
                        (which makes them tractable in the first place) does not map nicely onto deep learning frameworks
                        such as PyTorch or Tensorflow. Here we proposed a "smart" implementation of PCs, by squeezing all PC
                        operations into one a handful large <i>einsum operations</i>.
                        The result: dramatic speedups and memory savings, large-scale
                        generative modeling, including data imputation and outlier detection.
                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v119/peharz20a.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v119/peharz20a/peharz20a.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.com/38928148">Video</a> |
                        <a target="_blank" href="https://github.com/cambridge-mlg/EinsumNetworks">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>



                        <img id="dsmgp-pics" src="images/dsmgp_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <span class="label label-success">AISTATS'20</span>
                        <b>Deep Structured Mixtures of Gaussian Processes</b>
                        <br>
                        Martin Trapp, Robert Peharz, Franz Pernkopf, Carl Edward Rasmussen

                        <br>
                        <br>
                        Gaussian processes (GPs) are a powerful tool for Bayesian regression, as they represent a prior
                        over functions, which gets updated to a posterior via Bayesian inference.
                        Interestingly, this Bayesian update is tractable as it takes cubic time and quadratic memory.
                        While polynomial, this complexity is still prohibitive for large data, i.e. a few thousand data points.
                        Here we marry GPs with probabilistic circuits (PCs), yielding <i>Deep Structured Mixture of Gaussian Processes (DSMGPs),</i>
                        a new process model which elegantly mixes the tractable inference mechanisms of GPs and PCs.
                        The new model fits data better than several GP approximations while having comparable runtimes.
                        DSMGPs are also more data efficient than these approximate techniques and allow to model heteroscedastic noise.

                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v108/trapp20a.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v108/trapp20a/trapp20a.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.at/38930154">Video</a> |
                        <a target="_blank" href="https://github.com/trappmartin/DeepStructuredMixtures">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>




                        <img id="ps3-pics" src="images/ps3_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">ECML PKDD'20</span>
                        <b>PS3: Batch Mode Active Learning for Hate Speech Detection in Imbalanced Text Data</b>
                        <br>
                        Ricky Maulana Fajri, Samaneh Khoshrou, Robert Peharz, Mykola Pechenizkiy

                        <br>
                        <br>
                        The steadily growing prominence of social media exacerbates the problem of hostile
                        contents and hate-speech.
                        Automatically recognizing hate-speech is difficult, since the difference between hate-speech
                        and non-hate-speech might be subtle.
                        Moreover, hate-speech is relatively rare, leading to a highly class-skewed problem.
                        We developed PS3, a simple and effective batch mode active learning solution, which
                        updates the detection system by querying human domain-experts to annotate carefully selected
                        batches of data instances.
                        Despite its simplicity, PS3 sets state-of-the art on several hate-speech datasets.

                        <br>
                        <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-67670-4_5">Abstract</a> |
                        <a target="_blank" href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-67670-4_5.pdf">Paper</a> |
                        <a target="_blank" href="https://slideslive.com/38932384">Video</a> |
                        <a target="_blank" href="https://github.com/rmfajri/PS3">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>



                        <img id="spvae-pics" src="images/spvae_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">ICML'19</span>
                        <b>Hierarchical Decompositional Mixtures of Variational Autoencoders</b>
                        <br>
                        Ping Liang Tan, Robert Peharz

                        <br>
                        <br>
                        Variational autoencoders (VAEs) are simple and powerful neural density estimators and have
                        received a lot of attention recently.
                        However, inference and learning in VAEs is still challenging due to the intractable nature of the
                        model, especially in high dimensional data spaces.
                        Here we propose a <i>divide-and-conquer approach</i> and break up the overall density estimation
                        problem into many sub-problems, which are each modeled with a set of "small VAEs."
                        Learning and inference in these VAE components are orchestrated via probabilistic circuits
                        (PCs), yielding hierarchical decompositional mixtures of VAEs.
                        This novel model effectively uses <i>hybrid exact-approximate inference</i> (exact from PCs,
                        approximate from VAEs) in a natural way.
                        We show that our model outperforms classical VAEs on almost all of our experimental
                        benchmarks.
                        Moreover, we show that our model is highly data efficient and degrades very gracefully in
                        extremely low data regimes.


                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v97/tan19b.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v97/tan19b/tan19b.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/cambridge-mlg/SPVAE">Code</a>
                        <br>
                        <br>
                        <br>
                        <br>




                        <img id="ratspn-pics" src="images/ratspn_pics.jpg" width="600" border="0" height="315" alt="">
                        <br>
                        <br>
                        <span class="label label-success">UAI'19</span>
                        <b>Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning</b>
                        <br>
                        Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp, Kristian Kersting, Zoubin Ghahramani

                        <br>
                        <br>
                        Probabilistic circuits (PCs) such as sum-product networks (SPNs) are expressive probabilistic
                        models with a rich set of exact and efficient inference routines.
                        Their structure, however, does not easily map to deep learning frameworks such as Tensorflow.
                        Here we use an unspecialized random SPN structure which maps easily onto these frameworks and
                        can be scale to millions of parameters.
                        These <i>Random and Tensorized SPNs (RAT-SPNs)</i> perform often en par with state-of-the-art
                        neural net learners and deep neural networks on a diverse range of generative and discriminative
                        tasks.
                        RAT-SPNs can be used to naturally treat missing data and for outlier analysis and detection.
                        <br>
                        <a target="_blank" href="http://proceedings.mlr.press/v115/peharz20a.html">Abstract</a> |
                        <a target="_blank" href="http://proceedings.mlr.press/v115/peharz20a/peharz20a.pdf">Paper</a> |
                        <a target="_blank" href="https://github.com/cambridge-mlg/RAT-SPN">Code</a>


                    </div>
                </div>
            </div>
        </section>

        <!-- Academic Service Section -->
        <section  name="projects" id="academic_service">
            <div class="container lead">
                <div style="text-align: left; margin-top:10px" class="col-md-12">

                    <b class="noshow" style="display:none;" id="tab-8">
                        <button class="btn btn-default btn-lg btn-block" style="display: block; width: 100%;" onclick="javascript:$('#tab-academic-service-content').toggle();">Academic Service<span class="glyphicon glyphicon-chevron-down"></span></button>
                    </b>

                    <div class="allshow" style="display:none;" id="tab-academic-service-content">
                        <hr class="star-primary">

                        <h2>Academic Service</h2>

                        <br>

                        <ul>
                            <li>
                                <b>Area Chair</b><br>
                                <ul>
                                    <li>
                                        UAI (2022)
                                    </li>
                                    <li>
                                        ECML/PKDD (2022)
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <b>Senior Committee Member</b><br>
                                <ul>
                                    <li>
                                        UAI (2021)
                                    </li>
                                    <li>
                                        IJCAI (2019, 2020)
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <b>Reviewer</b><br>
                                <ul>
                                    <li>
                                        ICML (2013, 2014, 2019, 2020 [top 33%])
                                    </li>
                                    <li>
                                        NeurIPS (2018 [top 30%], 2019 [top 50%])
                                    </li>
                                    <li>
                                        AAAI (2019, 2021)
                                    </li>
                                    <li>
                                        IJCAI-ECAI (2018 [top 11.5%])
                                    </li>
                                    <li>
                                        UAI (2020)
                                    </li>
                                    <li>
                                        CVPR (2015, 2016, 2017, 2018)
                                    </li>
                                    <li>
                                        ICCV/ECCV (2015, 2016, 2018)
                                    </li>
                                    <li>
                                        PGM (2020)
                                    </li>
                                    <li>
                                        Interspeech, ICASSP (2013, 2014, 2016)
                                    </li>
                                </ul>
                            </li>

                            <li>
                                <b>Reviewed for Journals</b><br>
                                <ul>
                                    <li>
                                        Transactions on Artificial Intelligence
                                    </li>
                                    <li>
                                        Journal of Machine Learning Research
                                    </li>
                                    <li>
                                        IEEE Transactions on Audio, Speech and Language Processing
                                    </li>
                                    <li>
                                        Data Mining and Knowledge Discovery, Springer
                                    </li>
                                    <li>
                                        Machine Learning, Springer
                                    </li>
                                    <li>
                                        PeerJ
                                    </li>
                                    <li>
                                        Neural Computation, The MIT Press
                                    </li>
                                    <li>
                                        International Journal of Approximate Reasoning, Elsevier
                                    </li>
                                    <li>
                                        Expert Systems With Applications, Elsevier
                                    </li>
                                    <li>
                                        Neurocomputing, Elsevier
                                    </li>
                                    <li>
                                        Signal Processing, Elsevier
                                    </li>
                                    <li>
                                        Computational Optimization and Applications, Springer
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <b>Reviewed for Funding Agencies</b><br>
                                <ul>
                                    <li>
                                        Deutsche Forschungsgemeinschaft (DFG)
                                    </li>
                                </ul>
                            </li>
                        </ul>

                    </div>
                </div>
            </div>
        </section>

        <br>
        <br>
        <br>
        <br>
        <br>
        <hr class="star-primary">
        <footer>
            <small>
                <center>
                    © 2015 | D. Zeinalipour. Credits: AR template
                    <a onclick="javascript:$('#credit').toggle();"><img border="0" src="images/ccby.png"/></a>
                    <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
                        <a href="https://github.com/dmsl/academic-responsive-template" target="_blank">
                            https://github.com/dmsl/academic-responsive-template
                        </a> ]
                    </div>
                </center>
            </small>
        </footer>
        
    </body>
    
</html>
